{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install LightFM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJIvhkukm1xo",
        "outputId": "66742b05-38eb-42bf-8102-36ed83583128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting LightFM\n",
            "  Downloading lightfm-1.17.tar.gz (316 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/316.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/316.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.4/316.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from LightFM) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from LightFM) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from LightFM) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from LightFM) (1.5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->LightFM) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->LightFM) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->LightFM) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->LightFM) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->LightFM) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->LightFM) (3.5.0)\n",
            "Building wheels for collected packages: LightFM\n",
            "  Building wheel for LightFM (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for LightFM: filename=lightfm-1.17-cp310-cp310-linux_x86_64.whl size=808329 sha256=e4d26497026c5d19b9fef6066d0b6a81227e8d1ea052a89a1d9f843000eeff60\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/9b/7e/0b256f2168511d8fa4dae4fae0200fdbd729eb424a912ad636\n",
            "Successfully built LightFM\n",
            "Installing collected packages: LightFM\n",
            "Successfully installed LightFM-1.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files, drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4gIPJ6Osm8p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9ef95d-a3dc-44d0-dfe5-cabe139a0908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVR2jizimudY",
        "outputId": "9f84041a-9a03-481e-91ec-a4a3c79681d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n",
            "All rows in this batch are already processed. Moving to the next batch.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.sparse import coo_matrix, csr_matrix\n",
        "from lightfm import LightFM\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def train_and_recommend_batchwise_with_metadata(input_file, appmeta_file, output_file, rows_per_batch, k=5, num_epochs=10):\n",
        "    \"\"\"\n",
        "    Process the input dataset in batches, train LightFM, and append recommendations.\n",
        "    Incorporates app metadata into the recommendation pipeline.\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to the input CSV file (interaction data).\n",
        "        appmeta_file (str): Path to the app metadata CSV file.\n",
        "        output_file (str): Path to the output CSV file with recommendations.\n",
        "        rows_per_batch (int): Number of rows to process per batch.\n",
        "        k (int): Number of recommendations to generate for each user.\n",
        "        num_epochs (int): Number of epochs for LightFM training.\n",
        "    \"\"\"\n",
        "    # Load the app metadata\n",
        "    appmeta_data = pd.read_csv(appmeta_file)\n",
        "\n",
        "    # Preprocess app metadata\n",
        "    appmeta_data['price_category'] = appmeta_data['price'].apply(lambda x: 'Free' if x == 'Install' else 'Paid')\n",
        "    appmeta_data['num_reviews'] = appmeta_data['num_reviews'].replace(',', '', regex=True).astype(float)\n",
        "    appmeta_data['avg_rating'] = appmeta_data['avg_rating'].fillna(appmeta_data['avg_rating'].mean())\n",
        "\n",
        "    # Textual embedding for descriptions\n",
        "    vectorizer = TfidfVectorizer(max_features=500)\n",
        "    description_embeddings = vectorizer.fit_transform(appmeta_data['description'].fillna(''))\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    appmeta_features = pd.get_dummies(\n",
        "        appmeta_data[['app_package', 'app_category', 'content_rating', 'price_category']].fillna('Unknown'),\n",
        "        columns=['app_category', 'content_rating', 'price_category']\n",
        "    )\n",
        "    appmeta_features = pd.concat([appmeta_features, pd.DataFrame(description_embeddings.toarray())], axis=1)\n",
        "\n",
        "    # Load the dataset in chunks\n",
        "    for chunk in pd.read_csv(input_file, chunksize=rows_per_batch):\n",
        "        if os.path.exists(output_file):\n",
        "            processed_uids = set(pd.read_csv(output_file)['uid'])  # Load processed user IDs\n",
        "        else:\n",
        "            processed_uids = set()\n",
        "\n",
        "        chunk = chunk[~chunk['uid'].isin(processed_uids)]\n",
        "        if chunk.empty:\n",
        "            print(\"All rows in this batch are already processed. Moving to the next batch.\")\n",
        "            continue\n",
        "\n",
        "        # Map users and items to IDs\n",
        "        chunk['user_id'] = chunk['uid'].astype('category').cat.codes.astype(int)\n",
        "        chunk['item_id'] = chunk['app_package'].astype('category').cat.codes.astype(int)\n",
        "\n",
        "        # Create interaction weights\n",
        "        chunk['interaction_weight'] = chunk['rating'] * (1 + chunk['sentiment_score'])\n",
        "        chunk['interaction_weight'] = pd.to_numeric(chunk['interaction_weight'], errors='coerce').fillna(0)\n",
        "        chunk['interaction_weight'] = chunk['interaction_weight'].clip(lower=0)\n",
        "\n",
        "        # Build interaction matrix\n",
        "        interaction_matrix = coo_matrix(\n",
        "            (chunk['interaction_weight'], (chunk['user_id'], chunk['item_id'])),\n",
        "            shape=(chunk['user_id'].nunique(), chunk['item_id'].nunique())\n",
        "        )\n",
        "\n",
        "        # Check if the interaction matrix is empty\n",
        "        if interaction_matrix.nnz == 0:\n",
        "            print(\"No valid interactions in this batch. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        # Merge app metadata with the current chunk\n",
        "        merged_chunk = pd.merge(chunk, appmeta_features, on='app_package', how='left')\n",
        "\n",
        "        # Validate and build item features matrix\n",
        "        item_features_matrix = csr_matrix(\n",
        "            merged_chunk.select_dtypes(include=[np.number]).fillna(0).values\n",
        "        )\n",
        "\n",
        "        # Train LightFM\n",
        "        model = LightFM(loss='warp', no_components=20, random_state=42)\n",
        "        model.fit(interaction_matrix, item_features=item_features_matrix, epochs=num_epochs, num_threads=4)\n",
        "\n",
        "        # Create mapping from item IDs to app packages\n",
        "        item_mapping = dict(enumerate(chunk['app_package'].astype('category').cat.categories))\n",
        "\n",
        "        # Generate recommendations\n",
        "        def get_recommendations(user_id):\n",
        "            # Repeat user_id for all items\n",
        "            user_ids = np.full(interaction_matrix.shape[1], user_id, dtype=np.int32)\n",
        "            item_ids = np.arange(interaction_matrix.shape[1], dtype=np.int32)\n",
        "\n",
        "            scores = model.predict(\n",
        "                user_ids=user_ids,\n",
        "                item_ids=item_ids,\n",
        "                item_features=item_features_matrix\n",
        "            )\n",
        "            top_items = np.argsort(-scores)[:k]\n",
        "            return [item_mapping[item] for item in top_items]\n",
        "\n",
        "        unique_users = chunk['user_id'].unique()\n",
        "        recommendations = {user: get_recommendations(user) for user in unique_users}\n",
        "\n",
        "        # Add recommendations to the chunk\n",
        "        chunk['recommendations'] = chunk['user_id'].map(recommendations)\n",
        "\n",
        "        # Append results\n",
        "        mode = 'a' if os.path.exists(output_file) else 'w'\n",
        "        header = not os.path.exists(output_file)\n",
        "        chunk.to_csv(output_file, mode=mode, index=False, header=header)\n",
        "\n",
        "        print(f\"Processed and saved {len(chunk)} rows to {output_file}.\")\n",
        "\n",
        "# Main function to run the pipeline\n",
        "input_data = '/content/drive/MyDrive/DOCUMENTS_COLLEGE/Internships/Samsung_PRISM/Mobile_rec_dataset/game_data_sample_with_sentiment.csv'\n",
        "appmeta_data = '/content/drive/MyDrive/DOCUMENTS_COLLEGE/Internships/Samsung_PRISM/Mobile_rec_dataset/app_meta.csv'\n",
        "output_data = '/content/drive/MyDrive/DOCUMENTS_COLLEGE/Internships/Samsung_PRISM/Mobile_rec_dataset/game_data_with_recommendations.csv'\n",
        "rows_per_batch = 1000\n",
        "\n",
        "train_and_recommend_batchwise_with_metadata(input_data, appmeta_data, output_data, rows_per_batch, k=5, num_epochs=10)\n"
      ]
    },
    {
      "source": [],
      "cell_type": "code",
      "metadata": {
        "id": "AMF0PCj4WxN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wC0N-YqtNW71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}